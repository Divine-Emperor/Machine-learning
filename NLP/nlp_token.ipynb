{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bb2daefb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['natural', 'language', 'processing', '(', 'nlp', ')', 'is', 'exciting', ',', 'is', \"n't\", 'it', '?']\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "text = \"Natural Language Processing (NLP) is exciting, isn't it?\"\n",
    "tokens = word_tokenize(text.lower())\n",
    "\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "698a9cc7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt_tab to /home/harsh/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt_tab')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3138b904",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['natural', 'language', 'processing', '(', 'nlp', ')', 'is', 'exciting', ',', 'is', \"n't\", 'it', '?']\n",
      "['natural', 'language', 'processing', 'nlp', 'exciting']\n",
      "play\n",
      "play\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "text = \"Natural Language Processing (NLP) is exciting, isn't it?\"\n",
    "tokens = word_tokenize(text.lower())\n",
    "\n",
    "print(tokens)\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "filtered_tokens = [word for word in tokens if word.isalnum() and word not in stop_words]\n",
    "\n",
    "print(filtered_tokens)\n",
    "\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "\n",
    "stemmer = PorterStemmer()\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "print(stemmer.stem('playing'))   # play\n",
    "print(lemmatizer.lemmatize('playing', pos='v'))  # play"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "29e4386c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /home/harsh/nltk_data...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3d0b9ff",
   "metadata": {},
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "expected an indented block after function definition on line 5 (1001748078.py, line 6)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[5], line 6\u001b[0;36m\u001b[0m\n\u001b[0;31m    return BeautifulSoup(raw_html, \"html.parser\").get_text()\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mIndentationError\u001b[0m\u001b[0;31m:\u001b[0m expected an indented block after function definition on line 5\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Comprehensive Text Preprocessing Script\n",
    "Includes cleaning HTML, removing emojis, spelling correction,\n",
    "lowercasing, punctuation removal, tokenization, stopword removal,\n",
    "stemming, and lemmatization.\n",
    "\n",
    "Make sure to install dependencies:\n",
    "    pip install beautifulsoup4 emoji textblob nltk\n",
    "\n",
    "Run nltk.downloader once to avoid download prompts:\n",
    "    import nltk\n",
    "    nltk.download('punkt')\n",
    "    nltk.download('stopwords')\n",
    "    nltk.download('wordnet')\n",
    "\"\"\"\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "import emoji\n",
    "from textblob import TextBlob\n",
    "import string\n",
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "\n",
    "# Ensure required nltk data packages are downloaded\n",
    "nltk.download('punkt', quiet=True)\n",
    "nltk.download('stopwords', quiet=True)\n",
    "nltk.download('wordnet', quiet=True)\n",
    "\n",
    "def clean_html(raw_html: str) -> str:\n",
    "    \"\"\"Remove HTML tags from text.\"\"\"\n",
    "    return BeautifulSoup(raw_html, \"html.parser\").get_text()\n",
    "\n",
    "def remove_emojis(text: str) -> str:\n",
    "    \"\"\"Remove all emojis from text.\"\"\"\n",
    "    return emoji.replace_emoji(text, replace='')\n",
    "\n",
    "def correct_spelling(text: str) -> str:\n",
    "    \"\"\"Correct spelling mistakes using TextBlob.\"\"\"\n",
    "    return str(TextBlob(text).correct())\n",
    "\n",
    "def lower_case(text: str) -> str:\n",
    "    \"\"\"Convert text to lowercase.\"\"\"\n",
    "    return text.lower()\n",
    "\n",
    "def remove_punctuation(text: str) -> str:\n",
    "    \"\"\"Remove punctuation from text.\"\"\"\n",
    "    return text.translate(str.maketrans('', '', string.punctuation))\n",
    "\n",
    "def sentence_tokenize(text: str) -> list:\n",
    "    \"\"\"Split text into sentences.\"\"\"\n",
    "    return sent_tokenize(text)\n",
    "\n",
    "def word_tokenize_text(text: str) -> list:\n",
    "    \"\"\"Split text into words.\"\"\"\n",
    "    return word_tokenize(text)\n",
    "\n",
    "def remove_stopwords(words: list) -> list:\n",
    "    \"\"\"Remove English stopwords from a list of words.\"\"\"\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    return [w for w in words if w.lower() not in stop_words]\n",
    "\n",
    "def stem_words(words: list) -> list:\n",
    "    \"\"\"Stem words using PorterStemmer.\"\"\"\n",
    "    stemmer = PorterStemmer()\n",
    "    return [stemmer.stem(w) for w in words]\n",
    "\n",
    "def lemmatize_word(word: str, pos: str='n') -> str:\n",
    "    \"\"\"Lemmatize a single word with specified part of speech (default noun).\"\"\"\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    return lemmatizer.lemmatize(word, pos=pos)\n",
    "\n",
    "def main():\n",
    "    # Sample texts to demonstrate each function\n",
    "    html_text = \"<p>Hello <b>world</b>! üòä</p>\"\n",
    "    emoji_text = \"I ‚ù§Ô∏è Python üêç!\"\n",
    "    spelling_text = \"I havv a sleeppingg errr\"\n",
    "    mixed_text = \"Hello, world! It's awesome. NLP is fun and powerful!\"\n",
    "    words_list = ['this', 'is', 'a', 'sample', 'text', 'played', 'playing']\n",
    "\n",
    "    print(\"=== Clean HTML ===\")\n",
    "    print(clean_html(html_text))  # Hello world! üòä\n",
    "\n",
    "    print(\"\\n=== Remove Emojis ===\")\n",
    "    print(remove_emojis(emoji_text))  # I  Python !\n",
    "\n",
    "    print(\"\\n=== Correct Spelling ===\")\n",
    "    print(correct_spelling(spelling_text))  # I have a sleeping error\n",
    "\n",
    "    print(\"\\n=== Lowercase ===\")\n",
    "    print(lower_case(mixed_text))  # hello, world! it's awesome. nlp is fun and powerful!\n",
    "\n",
    "    print(\"\\n=== Remove Punctuation ===\")\n",
    "    print(remove_punctuation(mixed_text))  # Hello world Its awesome NLP is fun and powerful\n",
    "\n",
    "    print(\"\\n=== Sentence Tokenize ===\")\n",
    "    print(sentence_tokenize(mixed_text))  # ['Hello, world!', \"It's awesome.\", 'NLP is fun and powerful!']\n",
    "\n",
    "    print(\"\\n=== Word Tokenize ===\")\n",
    "    print(word_tokenize_text(mixed_text))  # ['Hello', ',', 'world', '!', 'It', \"'s\", 'awesome', '.', 'NLP', 'is', 'fun', 'and', 'powerful', '!']\n",
    "\n",
    "    print(\"\\n=== Remove Stopwords ===\")\n",
    "    filtered_words = remove_stopwords(words_list)\n",
    "    print(filtered_words)  # ['sample', 'text', 'played', 'playing']\n",
    "\n",
    "    print(\"\\n=== Stemming ===\")\n",
    "    stems = stem_words(words_list)\n",
    "    print(stems)  # e.g. ['thi', 'is', 'a', 'sampl', 'text', 'play', 'play']\n",
    "\n",
    "    print(\"\\n=== Lemmatization ===\")\n",
    "    # Lemmatize each word assuming noun (default)\n",
    "    lemmatized_words = [lemmatize_word(w) for w in words_list]\n",
    "    print(lemmatized_words)  # e.g. ['this', 'is', 'a', 'sample', 'text', 'played', 'playing']\n",
    "    # Lemmatize verb example\n",
    "    print(\"Lemmatized 'playing' as verb:\", lemmatize_word('playing', pos='v'))  # play\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "194fb45c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
